# Feature XGB Imputation Results:

all rand seeds set to 1776.


1) All available features:




    mod1_params <- list(
    "objective" = "reg:linear",
    "eval_metric" = "mae",
    "eta" = 0.01,
    "max_depth" = 4,  
    "subsample" = 0.9,
    "colsample_bytree" = 0.9,
    "lambda" = 0,
    "alpha" = 1,
    "max_delta_step" = 1,
    "nthread" = 6)


- cv test mae: 0.068845 with 687 iterations
- train mae: 0.067617 with 687 iterations
- holdout: 0.06695


top 20 features:
                           Feature        Gain        Cover   Frequency
 1:            tv_logreg_finsqft12 0.092062933 1.381528e-01 0.079124023
 2:    tv_logreg_structuretaxvalue 0.071159250 1.058326e-01 0.069549906
 3:          tv_logreg_lotsizesqft 0.070090653 7.393219e-03 0.042808408
 4:            tv_logreg_taxamount 0.063593776 1.075774e-01 0.062616925
 5:     tv_cat_building_age_five_3 0.063012878 3.685346e-04 0.004952129
 6:         tv_logreg_building_age 0.050455379 1.655158e-02 0.051612193
 7:             tv_logreg_taxvalue 0.036031821 5.923803e-02 0.035325190
 8:             tv_logreg_latitude 0.033568506 2.879757e-02 0.038406515
 9:            tv_logreg_longitude 0.026500383 3.930348e-03 0.030593155
10:     tv_cat_longitude_twenty_15 0.026027106 1.151842e-04 0.008143502
11:              tv_logreg_totsqft 0.024937252 6.714069e-02 0.034334764
12:      tv_cat_regionidcity_33612 0.020742581 2.809333e-03 0.008913833
13:         tv_logreg_landtaxvalue 0.019797191 3.743566e-02 0.032243865
14:      tv_cat_landtaxvalue_ten_5 0.017848164 2.116984e-06 0.009794212
15: tv_cat_airconditioningtypeid_1 0.017530565 1.391588e-05 0.007593265
16:            tv_logreg_finsqft15 0.013183685 4.002687e-02 0.022889843
17:  tv_logreg_censustractandblock 0.010332439 9.722953e-03 0.019918565
18:            tv_rawreg_finsqft12 0.009350327 1.326587e-02 0.005832508
19:            tv_rawreg_taxamount 0.008326056 1.013996e-02 0.006822934
20:          tv_rawreg_lotsizesqft 0.007606233 6.879898e-04 0.005722461



20 worst results from mod1:
        mod1_preds actuals           id residuals  abs_res
6047   0.001662046   4.737 pid_11617445 -4.735338 4.735338
6828   0.019225121   3.443 pid_11743456 -3.423775 3.423775
4814   0.015092999   3.160 pid_11427085 -3.144907 3.144907
11953  0.038228661   3.174 pid_12659338 -3.135771 3.135771
13159  0.010025442   2.987 pid_12847511 -2.976975 2.976975
9567  -0.013526559   2.745 pid_12224786 -2.758527 2.758527
867    0.010238081   2.623 pid_10823519 -2.612762 2.612762
11170  0.014291823   2.552 pid_12526165 -2.537708 2.537708
11130 -0.001905918   2.387 pid_12521809 -2.388906 2.388906
18645  0.016638190  -2.365 pid_14484884  2.381638 2.381638
5683   0.005842358  -2.375 pid_11563783  2.380842 2.380842
2426   0.017611980  -2.354 pid_11046267  2.371612 2.371612
20308  0.005371243  -2.354 pid_14717914  2.359371 2.359371
14560  0.006713361  -2.313 pid_13078609  2.319713 2.319713
21924  0.010651588   2.330 pid_17221678 -2.319348 2.319348
4924   0.012964249  -2.273 pid_11448482  2.285964 2.285964
2976   0.010709524  -2.273 pid_11118194  2.283710 2.283710
15656  0.009393066  -2.273 pid_14012870  2.282393 2.282393
3453   0.018469125  -2.263 pid_11173369  2.281469 2.281469
2252   0.017326623   2.292 pid_11014634 -2.274673 2.274673



2) Now... in a separate experiment. Predict the values of the top 5 features and
add those predictions (probabilities for each class in the case of categorical 
features) as features. I'm wondering if those will then show up within the top 
features and if the overall model statistics will improve.

